---
title: "Multivariate Data Analysis"
author: "Juan Pablo Zaldivar && Enric Millán && Joel Solé"
date: "2023-05-11"
output: html_document
---

```{r setup, include=F, warnings=F, echo=F}

knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo=FALSE)

YlGnBu = c("#ffffd9", "#edf8b1", "#c7e9b4", "#7fcdbb", "#41b6c4", "#1d91c0", "#225ea8", "#253494", "#081d58")

source("plots.R")

library(dplyr)
library(plotly)
library(ggplot2)
```

# Exploratory data analysis

The given dataset consists of measurements of various chemical properties of glass samples, along with their corresponding types. The dataset contains 10 features and 214 instances.

Each row in the dataset corresponds to a specific glass sample and provides the measurements of the aforementioned features, as well as the type of glass it belongs to.

```{r}
glass <- read.csv2("glass.csv", header=TRUE, sep=",")
head(glass)
```

```{r}
# Transform features into numeric
features <- names(glass)[names(glass) != "Type"]
glass[features] <- lapply(glass[features], as.numeric)

# Transform label as categorical
glass$Type <- as.factor(glass$Type)
```

The **type** variable is evidently categorical. It provides information about the different types of glass present in the database. There are six diffrenet levels (for a more detailed explanation of each level see the appendix).

```{r}
unique(glass$Type)
```

```{r}
bar_plot <- plot_ly(data = glass, x = ~Type,
                    color = ~Type, colors = "YlGnBu") %>%
  add_histogram() %>%
  layout(title = "Count of Types in glass Dataset",
         xaxis = list(title = "Type"), yaxis = list(title = "Count"))

bar_plot
```

The dataset is pretty unbalanced. The instances of **types** 1 and 2 constitute more than $67%$ of the glass types.

```{r}
# Compute correlation matrix
corr_matrix <- cor(glass[, 1:9])

# Create a heatmap using Plotly
heatmap <- plot_ly(
  x = colnames(corr_matrix),
  y = colnames(corr_matrix),
  z = corr_matrix,
  type = "heatmap",
  colorscale = "YlGnBu"
)

# Add cell values to the heatmap
heatmap <- heatmap %>%
  add_annotations(
    x = rep(colnames(corr_matrix), each = length(colnames(corr_matrix))),
    y = rep(colnames(corr_matrix), length(colnames(corr_matrix))),
    text = round(corr_matrix, 2),
    showarrow = FALSE)

heatmap
```

```{r}
histogram_plots(glass[, -c(10)])
```

Ba and Fe are not very common elements.

```{r}
box_plots(glass[, -c(10)])
```

# Preprocesing

## Data transformation

As seen in the exploratory analysis, the Ba and Fe elements are not very common among all instances. For that reason, we have chosen to binarize those variables, setting them to 1 if the dataframe instance contains Ba and Fe respectively (i.e. the value of the variables is greater than 0) and 0 otherwise.

```{r}
# Transform features into numeric
features <- names(glass)[names(glass) != "Type"]
glass[features] <- lapply(glass[features], as.numeric)

# Transform label as categorical
glass$Type <- as.factor(glass$Type)
```

## Missing values

```{r}
sum(is.na(glass))
```

## Duplicated values

The row number 40 is a duplicate of the previous row.

```{r}
anyDuplicated(glass)
glass <- glass[!duplicated(glass),]
```

```{r}
df_cat <- glass
# Categorize variable 'Ba'
df_cat$Ba <- ifelse(glass$Ba != 0.0, 1, 0)
df_cat$Ba <- as.factor(df_cat$Ba)

# Categorize variable 'Fe'
df_cat$Fe <- ifelse(glass$Fe != 0.0, 1, 0)
df_cat$Fe <- as.factor(df_cat$Fe)

df_cat$Type <- as.factor(df_cat$Type)
```


# Clusters

```{r}
df_num <- df_cat[, sapply(df_cat, is.numeric)]
```


```{r}
d <- dist(df_num, method = "euclidean") # distance matrix
fit <- hclust(d, method="single")
plot(fit,main="Dendrogram of Single Linkage") # Dendogram
```

```{r}
dim(d)
# d <- dist(df_cat, method = "manhattan") # distance matrixs
```

```{r}
head(df_num)
```



```{r}
fit1 <- hclust(d, method="complete") 
plot(fit1,main="Dendrogram of complete Linkage") # Dendogram
```

```{r}
fit2 <- hclust(d, method="average") 
plot(fit2,main="Dendrogram of Average Linkage") # Dendogram 
```

```{r}
fit3 <- hclust(d, method="ward.D2") 
plot(fit3,main="Dendrogram of Ward Method") # Dendogram 
```
Con el método Ward se observan claramente dos grupos, en caso de que consideremos altura 30. También se pueden considerar 3 grupos si usamos una altura menor. Esta clara diferenciación que no se ha visto con previos métodos se debe a que este método, a diferencia de los previos, es robusto ante la presencia de outliers que, como se ha visto anteriormente, aparecen en cantidades notables en este dataset.

```{r}
fit4 <- hclust(d, method="centroid") 
plot(fit4,main="Dendrogram of Centroid Method") # Dendogram 
```

### Highlighting groups

El método Ward es el que nos aporta una mayor claridad sobre los grupos que se pueden formar en el dataset, por tanto lo usaremos como base.

```{r}
plot(fit3,main="Dendrogram Ward Method Linkage")
groups <- cutree(fit3, k=2 )# c
rect.hclust(fit3, k=2, border="green")

groups <- cutree(fit3, k=3 )# c
rect.hclust(fit3, k=3, border="blue")

groups <- cutree(fit3, k=4 )# c
rect.hclust(fit3, k=4, border="purple")

groups <- cutree(fit3, k=6 )# c
rect.hclust(fit3, k=6, border="red")
# Con 5, Alaska se separa a un unico cluster.
```
Vemos que la opción más razonable parece ser la de 3 grupos, ya que las de 4 y 6 incluyen grupos muy reducidos, pero confirmemos esto con el Elbow Graph.

### Elbow Graph

```{r}
aux<-c()
for (i in 1:dim(df_num)[2]) {
  k<-kmeans(df_num,centers=i,nstart=25)
  aux[i]<-k$tot.withinss
}
plot(aux, xlab="Number of Clusters", ylab="TWSS", type="l", main="TWSS vs. number of clusters")
```
Observamos que el número óptimo de clústers es 3, tal y como habíamos teorizado,  pues en este se registra el cambio de pendiente más significativo. También hay otro cambio de pendiente con 4 grupos, pero este no parece ser tan importante, además en el dendrograma previo hemos podido observar que al hacer 4 grupos aparece uno con muy pocas muestras, cosa que no aporta gran claridad, y el resto son muy parecidos a los de 3 grupos.

```{r}
k3 <- kmeans(d, centers = 3, nstart = 25)
str(k3)
names(k3)

aggregate(df_num,by=list(k3$cluster),FUN=mean)
# Con k=3 se explican mejor los clusters
df_num$cluster<-as.numeric(k3$cluster)
```
It can be observed that the third group has a much higher value in _Mg_ and a much lower value in _Ba_ than the other two groups. Meanwhile, the first group has significantly higher values in _Al_, _K_ and _Ba_ but a significantly lower value in _Mg_. Finally, the second group does not outstand neither for lower nor for higher values, beign _K_ and _Fe_ exceptions where group two has the lowest values, although the difference is not so significant for _Fe_.

```{r}
### Sum of Squares ####
k3$withinss
# SS en cada cluster
k3$totss
k3$tot.withinss
# Suma de k4$withinss
k3$betweenss + k3$tot.withinss # BSS + Wssq
```
### Silhouette Index

```{r}
#### Silhoutte Index ######
library(cluster)
library(HSAUR)
library(kmed)

res <- fastkmed(d, 3)
silhouette <- sil(d, res$medoid, res$cluster)
#silhouette$result
silhouette$plot
```
```{r}
############ Group Means ##################
aggregate(df_num[,-11],by=list(res$cluster),FUN=mean)

## Comparison of Results of kmeans and kmedians
aggregate(df_num,by=list(k3$cluster),FUN=mean)
```

```{r}
## Classification of observations based on paritition ####
library(cluster)
names(k3)
clusplot(df_num, k3$cluster, color=TRUE, shade=TRUE,
         labels=2, lines=0)

c3<-clara(df_num,3)
names(c3)

clusplot(df_num, c3$cluster, color=TRUE, shade=TRUE,
         labels=2, lines=0)
```

The clusters formed with 'clara' are much clearer and well defined but there is still a mass of uncertain observations, around the values between 0 and 2 for component 1 and between -2 and 0 for component 2, that are mixed between two clusters.

```{r}
#### Clustering with Discriminant Projection Method #####

library(fpc)

plotcluster
plotcluster(df_num[,-10], as.integer(k3$cluster))
plotcluster(df_num[,-10],as.integer(c3$cluster))
```
This method appears to give a much clearer formation of clusters since the interaction between them isn't as significant as with previous methods. 
