---
title: "Multivariate Data Analysis"
author: "Juan Pablo Zaldivar && Enric Millán && Joel Solé"
date: "2023-05-11"
output: html_document
---

```{r setup, include=F, warnings=F, echo=F}

knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo=FALSE)

YlGnBu = c("#ffffd9", "#edf8b1", "#c7e9b4", "#7fcdbb", "#41b6c4", "#1d91c0", "#225ea8", "#253494", "#081d58")

source("plots.R")

library(dplyr)
library(plotly)
library(ggplot2)
library(factoextra)
library(ggpubr)
```

# Exploratory data analysis

The given dataset consists of measurements of various chemical properties of glass samples, along with their corresponding types. The dataset contains 10 features and 214 instances.

Each row in the dataset corresponds to a specific glass sample and provides the measurements of the aforementioned features, as well as the type of glass it belongs to.

```{r}
glass <- read.csv2("glass.csv", header=TRUE, sep=",")
head(glass)
```


```{r}
# Transform features into numeric
features <- names(glass)[names(glass) != "Type"]
glass[features] <- lapply(glass[features], as.numeric)

# Transform label as categorical
glass$Type <- as.factor(glass$Type)
```

The **type** variable is evidently categorical. It provides information about the different types of glass present in the database. There are six different levels (for a more detailed explanation of each level see the appendix).

```{r}
unique(glass$Type)
```

```{r}
bar_plot <- plot_ly(data = glass, x = ~Type,
                    color = ~Type, colors = "YlGnBu") %>%
  add_histogram() %>%
  layout(title = "Count of Types in glass Dataset",
         xaxis = list(title = "Type"), yaxis = list(title = "Count"))

bar_plot
```

The dataset is pretty unbalanced. The instances of **types** 1 and 2 constitute more than $67%$ of the glass types.

```{r}
# Compute correlation matrix
corr_matrix <- cor(glass[, 1:9])

# Create a heatmap using Plotly
heatmap <- plot_ly(
  x = colnames(corr_matrix),
  y = colnames(corr_matrix),
  z = corr_matrix,
  type = "heatmap",
  colorscale = "YlGnBu"
)

# Add cell values to the heatmap
heatmap <- heatmap %>%
  add_annotations(
    x = rep(colnames(corr_matrix), each = length(colnames(corr_matrix))),
    y = rep(colnames(corr_matrix), length(colnames(corr_matrix))),
    text = round(corr_matrix, 2),
    showarrow = FALSE)

heatmap
```

```{r}
histogram_plots(glass[, -c(10)])
```

Ba and Fe are not very common elements.

```{r}
box_plots(glass[, -c(10)])
```

# Preprocesing

## Data transformation

As seen in the exploratory analysis, the Ba and Fe elements are not very common among all instances. For that reason, we have chosen to binarize those variables, setting them to 1 if the dataframe instance contains Ba and Fe respectively (i.e. the value of the variables is greater than 0) and 0 otherwise.

```{r}
# Transform features into numeric
features <- names(glass)[names(glass) != "Type"]
glass[features] <- lapply(glass[features], as.numeric)

# Transform label as categorical
glass$Type <- as.factor(glass$Type)
```

## Missing values

```{r}
sum(is.na(glass))
```

## Duplicated values

The row number 40 is a duplicate of the previous row.

```{r}
anyDuplicated(glass)
glass <- glass[!duplicated(glass),]
```
## Binarization of Fe and Ba
```{r}
df_cat <- glass
# Categorize variable 'Ba'
df_cat$Ba <- ifelse(glass$Ba != 0.0, 1, 0)
df_cat$Ba <- as.factor(df_cat$Ba)

# Categorize variable 'Fe'
df_cat$Fe <- ifelse(glass$Fe != 0.0, 1, 0)
df_cat$Fe <- as.factor(df_cat$Fe)

df_cat$Type <- as.factor(df_cat$Type)
```


# Clusters

```{r}
df_num  <- subset(df_cat, select = -c(Ba, Fe, Type))
summary(df_num)
var(df_num$RI)
```
Since all variables that we are gonna use for clustering, except for RI(Reflextive Index), have the same units it is not advisable to scale them because that may hide some variance and inner groups. Moreover, reflective Index has a really low variance so it is probably not very important to make clusters.

```{r}
d <- dist(df_num, method = "euclidean") # distance matrix
fit <- hclust(d, method="single")
plot(fit,main="Dendrogram of Single Linkage", labels = FALSE) 
fit1 <- hclust(d, method="complete") 
plot(fit1,main="Dendrogram of complete Linkage", labels = FALSE) 
fit2 <- hclust(d, method="average") 
plot(fit2,main="Dendrogram of Average Linkage", labels = FALSE)
fit3 <- hclust(d, method="ward.D2") 
plot(fit3,main="Dendrogram of Ward Method", labels = FALSE) 
fit4 <- hclust(d, method="centroid") 
plot(fit4,main="Dendrogram of Centroid Method", labels = FALSE) # Dendogram 
```

Ward method seems to give the most reasonable clustering of the dataset. With this method, two distinct groups can be clearly observed when considering a height of 20. Additionally, three groups can be considered if we use a lower height, like 15. This clear differentiation, which was not seen with previous methods, is due to the robustness of this method in the presence of outliers and a lot of samples.


### Highlighting groups

The Ward method is the one that provides us with the greatest clarity regarding the groups that can be formed in the dataset. Therefore, we will use it as the basis for our analysis in hierarchical clustering.

```{r}
plot(fit3,main="Dendrogram Ward Method Linkage", labels=FALSE)
groups <- cutree(fit3, k=2 )# c
rect.hclust(fit3, k=2, border="green")

groups <- cutree(fit3, k=3 )# c
rect.hclust(fit3, k=3, border="blue")

groups <- cutree(fit3, k=4 )# c
rect.hclust(fit3, k=4, border="purple")

groups <- cutree(fit3, k=5 )# c
rect.hclust(fit3, k=5, border="orange")

groups <- cutree(fit3, k=6 )# c
rect.hclust(fit3, k=6, border="red")
# Con 5, Alaska se separa a un unico cluster.
```
Looks like the most reasonable option is 3 groups, since adding more groups creates some of them with very few samples, which may be due to the presence of some outliers. Nevertheless, let's confirm this hypothesis with the Elbow Graph.


### Elbow Graph

```{r}
aux<-c()
for (i in 1:dim(df_num)[2]) {
  k<-kmeans(df_num,centers=i,nstart=25)
  aux[i]<-k$tot.withinss
}
plot(aux, xlab="Number of Clusters", ylab="TWSS", type="l", main="TWSS vs. number of clusters")

fviz_nbclust(df_num, kmeans, method = "silhouette")
fviz_nbclust(df_num, kmeans, method = "gap_stat")
```
As wee can see, with different criteria: "wss", "silhouette" and "gap_stat", the result seems to be the same, the optimal number of clusters is 3 as we theorized previously. 


### Silhouette Index

```{r}
library(cluster)
library(HSAUR)
library(kmed)

res <- fastkmed(d, 6)
silhouette <- sil(d, res$medoid, res$cluster)
#silhouette$result
silhouette$plot
```
Silhouette index confirms that the most addequate number of clusters is 3.

```{r}
cluster = cutree(fit3,3)
# Create a bar plot of Type vs. Cluster
p <- ggplot(data = df_num, aes(x = cluster, fill = glass$Type)) +
  geom_bar() +
  theme_bw()

# Display the plot
print(p)
```
Keep in mind this distribution of types of glass around clusters because we will see that hclust cut in 3 groups coincides with kmeans for 3 groups.
## Kmeans
```{r}
k3 <- kmeans(d, centers = 3, nstart = 25)
str(k3)
names(k3)

aggregate(df_num,by=list(k3$cluster),FUN=mean)
df_num$cluster<-as.numeric(k3$cluster)
```

We observe that variables like _RI_, _Na_, _Al_ and _Si_ don't play an important role for differentiating the groups. On the other hand, variables such as _Mg_ outstand with a practically null value in group 1, a  moderate value in group 2, and a big value in comparion in group 3, being a variable that seems to be important for the clustering of the groups. Another variable important to separate groups might be _K_, which seems to sepparate group 1 from the other two. Something similar happens with _Ca_, which has a pretty high value for group 1 and similar values for groups 2 and 3.

```{r}
k3$withinss
k3$totss
k3$tot.withinss
k3$betweenss + k3$tot.withinss # BSS + Wss
```
### Kmeans Plots
```{r}
fviz_cluster(k3, data = df_num,
             palette = c("#2E9FDF", "#00AFBB", "#E7B800"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )
```

```{r}
## Classification of observations based on partition ####
library(cluster)
names(k3)
clusplot(df_num, k3$cluster, color=TRUE, shade=TRUE,
         labels=2, lines=0)

c3<-clara(df_num,3)
names(c3)

clusplot(df_num, c3$cluster, color=TRUE, shade=TRUE,
         labels=2, lines=0)
```

```{r}
fviz_cluster(k3, data = df_num,
             palette = c("#2E9FDF", "#00AFBB", "#E7B800"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw(),
             )

```

The clusters formed with kmeans seem to separate more properly the samples than the ones generated with 'clara' method. 

```{r}
# Create a bar plot of Type vs. Cluster
p <- ggplot(data = df_num, aes(x = k3$cluster, fill = glass$Type)) +
  geom_bar() +
  theme_bw()

# Display the plot
print(p)
```
Let's see if the clustering separates with clarity some of the different types of glass. Glass type 2, which goes for non-processed building windows, seems to be the most common in all groups, but for instance, group three gathers all of glass type 1 and 3, which are building and vehicle windows but most importantly both processed. Meanwhile group 2 contains most of types 5, 6 and 7, which are the types belonging to containers, tableware and headlamps, which happen to be the types less similar to vehicle or building windows. Cluster 1 seems to be the more diffuse, but we already saw in the cluster plot that is the one with less samples and is has a considerable intersection with group 2, so it makes sense that is the one that gives less information.

