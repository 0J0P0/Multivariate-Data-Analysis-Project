---
title: "Multivariate Data Analysis"
author: "Juan Pablo Zaldivar && Enric Millán && Joel Solé"
date: "2023-05-11"
output: html_document
---

```{r setup, include=F, warnings=F, echo=F}

knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo=FALSE)

YlGnBu = c("#ffffd9", "#edf8b1", "#c7e9b4", "#7fcdbb", "#41b6c4", "#1d91c0", "#225ea8", "#253494", "#081d58")

source("plots.R")

library(dplyr)
library(plotly)
library(ggplot2)
```

# Exploratory data analysis

```{r}
df <- read.csv2("glass.csv", header=TRUE, sep=",")
head(df)
```

```{r}
# Transform features into numeric
features <- names(df)[names(df) != "Type"]
df[features] <- lapply(df[features], as.numeric)

# Transform label as categorical
df$Type <- as.factor(df$Type)
```

```{r}
unique(df$Type)
```

```{r}
bar_plot <- plot_ly(data = df, x = ~Type,
                    color = ~Type, colors = "YlGnBu") %>%
  add_histogram() %>%
  layout(title = "Count of Types in df Dataset",
         xaxis = list(title = "Type"), yaxis = list(title = "Count"))

bar_plot
```

```{r}
# Compute correlation matrix
corr_matrix <- cor(df[, 1:9])

# Create a heatmap using Plotly
heatmap <- plot_ly(
  x = colnames(corr_matrix),
  y = colnames(corr_matrix),
  z = corr_matrix,
  type = "heatmap",
  colorscale = "YlGnBu"
)

# Add cell values to the heatmap
heatmap <- heatmap %>%
  add_annotations(
    x = rep(colnames(corr_matrix), each = length(colnames(corr_matrix))),
    y = rep(colnames(corr_matrix), length(colnames(corr_matrix))),
    text = round(corr_matrix, 2),
    showarrow = FALSE)

heatmap
```

```{r}
histogram_plots(df, ncol(df))
```

Ba and Fe are not very common elements.

```{r}
box_plots(df, ncol(df))
```


# Preprocesing

## Missing values

```{r}
sum(is.na(df))
```

## Duplicated values

The row number 40 is a duplicate of the previous row.

```{r}
anyDuplicated(df)
df <- df[!duplicated(df),]
```

## Data transformation

```{r}
# Transform features into numeric
features <- names(df)[names(df) != "Type"]
df[features] <- lapply(df[features], as.numeric)

# Transform label as categorical
df$Type <- as.factor(df$Type)
```

As seen in the exploratory analysis, the Ba and Fe elements are not very common among all instances. For that reason, we have chosen to binarize those variables, setting them to 1 if the df instance contains Ba and Fe respectively (i.e. the value of the variables is greater than 0) and 0 otherwise.

```{r}
df_num <- df
# Categorize variable 'Ba'
df$Ba <- ifelse(df$Ba != 0.0, 1, 0)

# Categorize variable 'Fe'
df$Fe <- ifelse(df$Fe != 0.0, 1, 0)

df_cat <- df
```



## Clusters

```{r}
d <- dist(df_cat, method = "euclidean") # distance matrix
fit <- hclust(d, method="single") 
plot(fit,main="Dendrogram of Single Linkage") # Dendogram
```

```{r}
fit1 <- hclust(d, method="complete") 
plot(fit1,main="Dendrogram of complete Linkage") # Dendogram
```

```{r}
fit2 <- hclust(d, method="average") 
plot(fit2,main="Dendrogram of Average Linkage") # Dendogram 
```

```{r}
fit3 <- hclust(d, method="ward.D2") 
plot(fit3,main="Dendrogram of Ward Method") # Dendogram 
```
With the Ward method, two distinct groups are clearly observed when considering a height of 30. If we use a lower height, we can also consider three groups. This clear differentiation, which was not seen with previous methods, is due to the fact that this method, unlike the previous ones, is robust in the presence of outliers. As seen in the EDA, the bar plots showed some possible outliers so that would explain why this method works better.

```{r}
fit4 <- hclust(d, method="centroid") 
plot(fit4,main="Dendrogram of Centroid Method") # Dendogram 
```

### Highlighting groups

The Ward method is the one that provides us with the greatest clarity regarding the groups that can be formed in the dataset; therefore, we will use it from now on.

```{r}
plot(fit3,main="Dendrogram Ward Method Linkage")
groups <- cutree(fit3, k=2 )# c
rect.hclust(fit3, k=2, border="green")

groups <- cutree(fit3, k=3 )# c
rect.hclust(fit3, k=3, border="blue")

groups <- cutree(fit3, k=4 )# c
rect.hclust(fit3, k=4, border="purple")

groups <- cutree(fit3, k=6 )# c
rect.hclust(fit3, k=6, border="red")
# Con 5, Alaska se separa a un unico cluster.
```
We see that the most reasonable option appears to be the one with 3 groups, as the ones with 4 and 6 include very small groups. However, let's confirm this with the Elbow Graph.

### Elbow Graph

```{r}
aux<-c()
for (i in 2:6) {
  k<-kmeans(df_cat,centers=i,nstart=25)
  aux[i]<-k$tot.withinss
}
plot(aux, xlab="Number of Clusters", ylab="TWSS", type="l", main="TWSS vs. number of clusters")
```
We observe that the optimal number of clusters is 3, as we had theorized, as it shows the most significant change in slope. There is also another change in slope with 4 groups, but it does not appear to be as important. Additionally, in the previous dendrogram, we were able to observe that when creating 4 groups, one of them contains very few samples, which does not provide much clarity, and the remaining groups are very similar to those of the 3-group solution.

```{r}
k3 <- kmeans(d, centers = 3, nstart = 25)
str(k3)
names(k3)

aggregate(df_cat,by=list(k3$cluster),FUN=mean)
# Con k=3 se explican mejor los clusters
df_cat$cluster<-as.numeric(k3$cluster)
```
It can be observed that the first group has a much higher value in _Mg_ and a much lower value in _Ba_ than the other two groups. Meanwhile, the second group has significantly higher values in  _K_ and _Ca_ but a practically null value in _Mg_. Finally, the third group does not outstand neither for much lower nor for much higher values than the other two, being _K_ and _Fe_ exceptions where group two has the lowest values, although the difference is not so significant as already mentioned.

```{r}
### Sum of Squares ####
k3$withinss
# SS en cada cluster
k3$totss
k3$tot.withinss
# Suma de k4$withinss
k3$betweenss + k3$tot.withinss # BSS + Wssq
```
### Silhouette Index

```{r}
#### Silhoutte Index ######
library(cluster)
library(HSAUR)
library(kmed)

res <- fastkmed(d, 3)
silhouette <- sil(d, res$medoid, res$cluster)
#silhouette$result
silhouette$plot
```
```{r}
############ Group Means ##################
aggregate(df_cat[,-11],by=list(res$cluster),FUN=mean)

## Comparison of Results of kmeans and kmedians
aggregate(df_cat,by=list(k3$cluster),FUN=mean)
```

```{r}
## Classification of observations based on paritition ####
library(cluster)
names(k3)
clusplot(df_cat, k3$cluster, color=TRUE, shade=TRUE,
         labels=2, lines=0)

c3<-clara(df_cat,3)
names(c3)

clusplot(df_cat, c3$cluster, color=TRUE, shade=TRUE,
         labels=2, lines=0)
```

The clusters formed with 'kmeans' are much clearer and well defined but there is still a mass of uncertain observations. Those fall between group 2 and 3, specifically around -3 in component 1 and 2 in component 2.

```{r}
#### Clustering with Discriminant Projection Method #####

library(fpc)

plotcluster
plotcluster(df_cat[,-10], as.integer(k3$cluster))
plotcluster(df_cat[,-10],as.integer(c3$cluster))
```
With discriminant projection method, the 'clara' clustering improves a lot. There's three clearly differentiated clusters and only 2 or 3 samples are out of place.
