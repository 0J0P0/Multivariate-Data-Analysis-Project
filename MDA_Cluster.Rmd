---
title: "Multivariate Data Analysis"
author: "Juan Pablo Zaldivar && Enric Millán && Joel Solé"
date: "2023-05-11"
output: html_document
---

```{r setup, include=F, warnings=F, echo=F}

knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo=FALSE)

YlGnBu = c("#ffffd9", "#edf8b1", "#c7e9b4", "#7fcdbb", "#41b6c4", "#1d91c0", "#225ea8", "#253494", "#081d58")

source("plots.R")

library(dplyr)
library(plotly)
library(ggplot2)
```

# Exploratory data analysis

```{r}
df <- read.csv2("glass.csv", header=TRUE, sep=",")
head(df)
```

```{r}
# Transform features into numeric
features <- names(df)[names(df) != "Type"]
df[features] <- lapply(df[features], as.numeric)

# Transform label as categorical
df$Type <- as.factor(df$Type)
```

```{r}
unique(df$Type)
```

```{r}
bar_plot <- plot_ly(data = df, x = ~Type,
                    color = ~Type, colors = "YlGnBu") %>%
  add_histogram() %>%
  layout(title = "Count of Types in df Dataset",
         xaxis = list(title = "Type"), yaxis = list(title = "Count"))

bar_plot
```

```{r}
# Compute correlation matrix
corr_matrix <- cor(df[, 1:9])

# Create a heatmap using Plotly
heatmap <- plot_ly(
  x = colnames(corr_matrix),
  y = colnames(corr_matrix),
  z = corr_matrix,
  type = "heatmap",
  colorscale = "YlGnBu"
)

# Add cell values to the heatmap
heatmap <- heatmap %>%
  add_annotations(
    x = rep(colnames(corr_matrix), each = length(colnames(corr_matrix))),
    y = rep(colnames(corr_matrix), length(colnames(corr_matrix))),
    text = round(corr_matrix, 2),
    showarrow = FALSE)

heatmap
```

```{r}
histogram_plots(df, ncol(df))
```

Ba and Fe are not very common elements.

```{r}
box_plots(df, ncol(df))
```


# Preprocesing

## Data transformation

```{r}
# Transform features into numeric
features <- names(df)[names(df) != "Type"]
df[features] <- lapply(df[features], as.numeric)

# Transform label as categorical
df$Type <- as.factor(df$Type)
```

As seen in the exploratory analysis, the Ba and Fe elements are not very common among all instances. For that reason, we have chosen to binarize those variables, setting them to 1 if the df instance contains Ba and Fe respectively (i.e. the value of the variables is greater than 0) and 0 otherwise.

```{r}
df_num <- df
# Categorize variable 'Ba'
df$Ba <- ifelse(df$Ba != 0.0, 1, 0)

# Categorize variable 'Fe'
df$Fe <- ifelse(df$Fe != 0.0, 1, 0)

df_cat <- df
```


## Missing values

```{r}
sum(is.na(df))
```

## Duplicated values

The row number 40 is a duplicate of the previous row.

```{r}
anyDuplicated(df)
df <- df[!duplicated(df),]
```

## Clusters

```{r}
d <- dist(df_num, method = "euclidean") # distance matrix
fit <- hclust(d, method="single") 
plot(fit,main="Dendrogram of Single Linkage") # Dendogram
```

```{r}
fit1 <- hclust(d, method="complete") 
plot(fit1,main="Dendrogram of complete Linkage") # Dendogram
```

```{r}
fit2 <- hclust(d, method="average") 
plot(fit2,main="Dendrogram of Average Linkage") # Dendogram 
```

```{r}
fit3 <- hclust(d, method="ward.D2") 
plot(fit3,main="Dendrogram of Ward Method") # Dendogram 
```
Con el método Ward se observan claramente dos grupos, en caso de que consideremos altura 30. También se pueden considerar 3 grupos si usamos una altura menor. Esta clara diferenciación que no se ha visto con previos métodos se debe a que este método, a diferencia de los previos, es robusto ante la presencia de outliers que, como se ha visto anteriormente, aparecen en cantidades notables en este dataset.

```{r}
fit4 <- hclust(d, method="centroid") 
plot(fit4,main="Dendrogram of Centroid Method") # Dendogram 
```

### Highlighting groups

El método Ward es el que nos aporta una mayor claridad sobre los grupos que se pueden formar en el dataset, por tanto lo usaremos como base.

```{r}
plot(fit3,main="Dendrogram Ward Method Linkage")
groups <- cutree(fit3, k=2 )# c
rect.hclust(fit3, k=2, border="green")

groups <- cutree(fit3, k=3 )# c
rect.hclust(fit3, k=3, border="blue")

groups <- cutree(fit3, k=4 )# c
rect.hclust(fit3, k=4, border="purple")

groups <- cutree(fit3, k=6 )# c
rect.hclust(fit3, k=6, border="red")
# Con 5, Alaska se separa a un unico cluster.
```
Vemos que la opción más razonable parece ser la de 3 grupos, ya que las de 4 y 6 incluyen grupos muy reducidos, pero confirmemos esto con el Elbow Graph.

### Elbow Graph

```{r}
aux<-c()
for (i in 2:6) {
  k<-kmeans(df_num,centers=i,nstart=25)
  aux[i]<-k$tot.withinss
}
plot(aux, xlab="Number of Clusters", ylab="TWSS", type="l", main="TWSS vs. number of clusters")
```
Observamos que el número óptimo de clústers es 3, tal y como habíamos teorizado,  pues en este se registra el cambio de pendiente más significativo. También hay otro cambio de pendiente con 4 grupos, pero este no parece ser tan importante, además en el dendrograma previo hemos podido observar que al hacer 4 grupos aparece uno con muy pocas muestras, cosa que no aporta gran claridad, y el resto son muy parecidos a los de 3 grupos.

```{r}
k3 <- kmeans(d, centers = 3, nstart = 25)
str(k3)
names(k3)

aggregate(df_num,by=list(k3$cluster),FUN=mean)
# Con k=3 se explican mejor los clusters
df_num$cluster<-as.numeric(k3$cluster)
```
It can be observed that the third group has a much higher value in _Mg_ and a much lower value in _Ba_ than the other two groups. Meanwhile, the first group has significantly higher values in _Al_, _K_ and _Ba_ but a significantly lower value in _Mg_. Finally, the second group does not outstand neither for lower nor for higher values, beign _K_ and _Fe_ exceptions where group two has the lowest values, although the difference is not so significant for _Fe_.

```{r}
### Sum of Squares ####
k3$withinss
# SS en cada cluster
k3$totss
k3$tot.withinss
# Suma de k4$withinss
k3$betweenss + k3$tot.withinss # BSS + Wssq
```
### Silhouette Index

```{r}
#### Silhoutte Index ######
library(cluster)
library(HSAUR)
library(kmed)

res <- fastkmed(d, 3)
silhouette <- sil(d, res$medoid, res$cluster)
#silhouette$result
silhouette$plot
```
```{r}
############ Group Means ##################
aggregate(df_num[,-11],by=list(res$cluster),FUN=mean)

## Comparison of Results of kmeans and kmedians
aggregate(df_num,by=list(k3$cluster),FUN=mean)
```

```{r}
## Classification of observations based on paritition ####
library(cluster)
names(k3)
clusplot(df_num, k3$cluster, color=TRUE, shade=TRUE,
         labels=2, lines=0)

c3<-clara(df_num,3)
names(c3)

clusplot(df_num, c3$cluster, color=TRUE, shade=TRUE,
         labels=2, lines=0)
```

The clusters formed with 'clara' are much clearer and well defined but there is still a mass of uncertain observations, around the values between 0 and 2 for component 1 and between -2 and 0 for component 2, that are mixed between two clusters.

```{r}
#### Clustering with Discriminant Projection Method #####

library(fpc)

plotcluster
plotcluster(df_num[,-10], as.integer(k3$cluster))
plotcluster(df_num[,-10],as.integer(c3$cluster))
```
This method appears to give a much clearer formation of clusters since the interaction between them isn't as significant as with previous methods. 
